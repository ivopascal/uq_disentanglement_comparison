{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cbd17b-ee0b-4ff5-9318-ef8341e31265",
   "metadata": {},
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras_uncertainty.layers import StochasticDropout, SamplingSoftmax\n",
    "from keras_uncertainty.models import DisentangledStochasticClassifier, StochasticClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras_uncertainty.utils import numpy_entropy\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa6c174-fc15-45b2-a3d8-7c6a7d24c3e5",
   "metadata": {},
   "source": [
    "X, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1.5, 1.5],[0, -1.5]], random_state=0)\n",
    "BATCH_SIZE = 256\n",
    "NUM_SAMPLES = 100"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe652f14-3dc5-42a2-b0dd-04bea14b02e5",
   "metadata": {},
   "source": [
    "def uncertainty(probs):\n",
    "    return numpy_entropy(probs, axis=-1)\n",
    "\n",
    "\n",
    "def two_head_model(trunk_model, num_classes=2, num_samples=100):\n",
    "    inp = Input(shape=(2,))\n",
    "    x = trunk_model(inp)\n",
    "    logit_mean = Dense(num_classes, activation=\"linear\")(x)\n",
    "    logit_var = Dense(num_classes, activation=\"softplus\")(x)\n",
    "    probs = SamplingSoftmax(num_samples=num_samples, variance_type=\"linear_std\")([logit_mean, logit_var])\n",
    "    \n",
    "    train_model = Model(inp, probs, name=\"train_model\")\n",
    "    pred_model = Model(inp, [logit_mean, logit_var], name=\"pred_model\")\n",
    "\n",
    "    train_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return train_model, pred_model\n",
    "\n",
    "\n",
    "def train_stochastic_model(trunk_model, x_train, y_train, epochs=300):\n",
    "    train_model, pred_model = two_head_model(trunk_model)\n",
    "    train_model.fit(x_train, y_train, verbose=2, epochs=epochs, batch_size=BATCH_SIZE)\n",
    "\n",
    "    fin_model = DisentangledStochasticClassifier(pred_model, epi_num_samples=NUM_SAMPLES)\n",
    "\n",
    "    return fin_model\n",
    "\n",
    "def eval_disentangled_model(disentangled_model, samples):\n",
    "    pred_mean, pred_ale_std, pred_epi_std = disentangled_model.predict(samples, batch_size=BATCH_SIZE)\n",
    "    ale_entropy = uncertainty(pred_ale_std)\n",
    "    epi_entropy = uncertainty(pred_epi_std)\n",
    "\n",
    "    return ale_entropy, epi_entropy\n",
    "\n",
    "def train_disentangling_dropout_model(x_train, y_train, prob=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation=\"relu\", input_shape=(2,)))\n",
    "    model.add(StochasticDropout(prob))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(StochasticDropout(prob))\n",
    "\n",
    "    return train_stochastic_model(model, x_train, y_train, epochs=50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66ec68e-bf5c-4588-bf22-f6f685bebc6c",
   "metadata": {},
   "source": [
    "disentangled_dropout_model = train_disentangling_dropout_model(X, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c82e17e-5591-41c2-92bd-d5751e9f0623",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "min_x, max_x = [-30, -30] , [30, 30]\n",
    "res = 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(min_x[0], max_x[0], res), np.arange(min_x[1], max_x[1], res))\n",
    "domain = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "print(domain.shape)\n",
    "\n",
    "ale_entropy, epi_entropy = eval_disentangled_model(disentangled_dropout_model, domain)\n",
    "ale_entropy = ale_entropy.reshape(xx.shape)\n",
    "epi_entropy = epi_entropy.reshape(xx.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299eadec-4c99-4d13-8d72-36460bcdbb85",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = pl.cm.binary\n",
    "my_cmap = cmap(np.arange(cmap.N))\n",
    "#my_cmap[:, 0] = 0.0\n",
    "my_cmap[:, -1] = 0.7\n",
    "my_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "\n",
    "fig, axes =  plt.subplots(ncols=1, nrows=2, figsize=(6, 10), squeeze=False)    \n",
    "ax_ale = axes[0][0]\n",
    "ax_epi = axes[1][0]\n",
    "\n",
    "cf_ale = ax_ale.contourf(xx, yy, ale_entropy, antialiased=True)\n",
    "ax_ale.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap)\n",
    "ax_ale.get_xaxis().set_ticks([])\n",
    "ax_ale.get_yaxis().set_ticks([])\n",
    "ax_ale.autoscale(False)\n",
    "\n",
    "cf_epi = ax_epi.contourf(xx, yy, epi_entropy, antialiased=True)\n",
    "ax_epi.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap)\n",
    "ax_epi.get_xaxis().set_ticks([])\n",
    "ax_epi.get_yaxis().set_ticks([])\n",
    "ax_epi.autoscale(False)\n",
    "\n",
    "ax_ale.set_ylabel(\"Aleatoric\")\n",
    "ax_epi.set_ylabel(\"Epistemic\")\n",
    "ax_ale.set_title(\"Multi-Head Disentangled MC-Dropout\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "290efecd-9aec-4faa-b113-d9f1dc9f122b",
   "metadata": {},
   "source": [
    "def evaluate_entropy_dropout_model(entropy_model, samples, num_samples=NUM_SAMPLES):\n",
    "    preds = entropy_model.predict(samples, num_samples=num_samples)\n",
    "    print(preds.shape)\n",
    "    entropy = uncertainty(preds)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def train_entropy_dropout_model(x_train, y_train, prob=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation=\"relu\", input_shape=(2,)))\n",
    "    model.add(StochasticDropout(prob))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(StochasticDropout(prob))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.fit(x_train, y_train, verbose=2, epochs=50)\n",
    "    \n",
    "    mc_model = StochasticClassifier(model)\n",
    "\n",
    "    return mc_model\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a2943a22-7255-445a-be65-d08af573de1c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "entropy_model = train_entropy_dropout_model(X, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "451a4b19-1b96-412e-bad8-b800d94bb2b5",
   "metadata": {},
   "source": [
    "individual_predictions = entropy_model.predict_samples(domain, num_samples=NUM_SAMPLES)\n",
    "individual_predictions.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "7b05a143-86d9-4d20-8e04-f4ceae13e922",
   "metadata": {},
   "source": [
    "from keras import activations\n",
    "\n",
    "# Here we try something new: what if we apply this entropy formulation directly on the logits somehow\n",
    "# I found this has problems\n",
    "# The entropy formulation no-longer works, because log of negative is NaN\n",
    "# I am not able to find an alternative approach to this\n",
    "\n",
    "entropy_model.model.layers[-1].activation = activations.linear\n",
    "entropy_model.model.compile(loss=\"sparse_categorical_crossentropy\")\n",
    "individual_predictions = entropy_model.predict_samples(domain, num_samples=NUM_SAMPLES)\n",
    "original_individual_predictions = individual_predictions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "aa7808f8-108f-44db-9cfb-6e99cdeede77",
   "metadata": {},
   "source": [
    "import keras_uncertainty.backend as K\n",
    "\n",
    "individual_predictions = K.softmax(K.constant(original_individual_predictions), axis=-1).numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "a3dde5d8-2b39-4e20-b8fd-2b924d92f3a2",
   "metadata": {},
   "source": [
    "def predictive_entropy(probs, axis=-1, eps=1e-6):\n",
    "    probs = np.mean(probs, axis=0)\n",
    "    return -np.sum(probs * np.log(probs + eps), axis=axis)\n",
    "\n",
    "def shannon_entropy(probs, eps=1e-6):\n",
    "  return -np.mean((probs * np.log(probs + eps)).sum(axis=-1), axis=0)\n",
    "\n",
    "def mutual_information(probs):\n",
    "  return predictive_entropy(probs) - shannon_entropy(probs)\n",
    "\n",
    "def logit_variance(logits):\n",
    "    return np.var(logits, axis=0).mean(axis=-1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "2961e866-2e04-4f46-9bcd-1c8a3d488787",
   "metadata": {},
   "source": [
    "fig, axes =  plt.subplots(ncols=1, nrows=4, figsize=(6, 10), squeeze=False)    \n",
    "ax_pred = axes[0][0]\n",
    "ax_exp = axes[1][0]\n",
    "ax_mi = axes[2][0]\n",
    "ax_var = axes[3][0]\n",
    "\n",
    "\n",
    "\n",
    "pred_entr = predictive_entropy(individual_predictions)\n",
    "pred_entr = pred_entr.reshape(xx.shape)\n",
    "cf_pred = ax_pred.contourf(xx, yy, pred_entr, antialiased=True)\n",
    "ax_pred.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap)\n",
    "ax_pred.get_xaxis().set_ticks([])\n",
    "ax_pred.get_yaxis().set_ticks([])\n",
    "ax_pred.autoscale(False)\n",
    "\n",
    "exp_entr = expected_entropy(individual_predictions)\n",
    "exp_entr = exp_entr.reshape(xx.shape)\n",
    "cf_exp = ax_exp.contourf(xx, yy, exp_entr, antialiased=True)\n",
    "ax_exp.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap)\n",
    "ax_exp.get_xaxis().set_ticks([])\n",
    "ax_exp.get_yaxis().set_ticks([])\n",
    "ax_exp.autoscale(False)\n",
    "\n",
    "\n",
    "mut_inf = mutual_information(individual_predictions)\n",
    "mut_inf = mut_inf.reshape(xx.shape)\n",
    "cf_mi = ax_mi.contourf(xx, yy, mut_inf, antialiased=True)\n",
    "ax_mi.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap)\n",
    "ax_mi.get_xaxis().set_ticks([])\n",
    "ax_mi.get_yaxis().set_ticks([])\n",
    "ax_mi.autoscale(False)\n",
    "\n",
    "\n",
    "pred_var = np.std(individual_predictions, axis=0).mean(axis=-1)\n",
    "pred_var = pred_var.reshape(xx.shape)\n",
    "cf_var = ax_var.contourf(xx, yy, pred_var, antialiased=True)\n",
    "ax_var.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap)\n",
    "ax_var.get_xaxis().set_ticks([])\n",
    "ax_var.get_yaxis().set_ticks([])\n",
    "ax_var.autoscale(False)\n",
    "\n",
    "\n",
    "\n",
    "ax_pred.set_ylabel(\"Predictive Entropy (mix)\")\n",
    "ax_exp.set_ylabel(\"Expected Entropy (ale)\")\n",
    "ax_mi.set_ylabel(\"Mutual Information (epi)\")\n",
    "ax_var.set_ylabel(\"Probability Variance (epi)\")\n",
    "\n",
    "\n",
    "ax_pred.set_title(\"Entropy disentangled MC-Dropout\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "86b088b5-2daf-486e-8aa2-232ad4963e01",
   "metadata": {},
   "source": [
    "np.var(individual_predictions, axis=0).shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "44c885c3-a852-4d1d-baae-ac513936c584",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "disentangling_accuracies = []\n",
    "disentangling_aleatorics = []\n",
    "disentangling_epistemics = []\n",
    "\n",
    "entropy_accuracies = []\n",
    "entropy_aleatorics = []\n",
    "entropy_epistemics = []\n",
    "X_test, y_test = make_blobs(n_samples=500, n_features=2, centers=[[-1.5, 1.5],[0, -1.5]], random_state=0)\n",
    "dataset_sizes = np.logspace(start=1, stop=10, base=2, num=20) \n",
    "\n",
    "for dataset_size in tqdm(dataset_sizes):\n",
    "\n",
    "    X_train, y_train = make_blobs(n_samples=int(dataset_size), n_features=2, centers=[[-1.5, 1.5],[0, -1.5]], random_state=0)\n",
    "    \n",
    "    \n",
    "    disentangle_model = train_disentangling_dropout_model(X_train, y_train)\n",
    "    entropy_model = train_entropy_dropout_model(X_train, y_train)\n",
    "\n",
    "    pred_mean, pred_ale_std, pred_epi_std = disentangle_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "    entropy_preds = entropy_model.predict_samples(X_test, num_samples=NUM_SAMPLES, batch_size=BATCH_SIZE)\n",
    "    disentangling_accuracies.append(accuracy_score(y_test, pred_mean.argmax(axis=1)))\n",
    "    disentangling_aleatorics.append(uncertainty(pred_ale_std).mean())\n",
    "    disentangling_epistemics.append(uncertainty(pred_epi_std).mean())\n",
    "\n",
    "    entropy_accuracies.append(accuracy_score(y_test, entropy_preds.mean(axis=0).argmax(axis=1)))\n",
    "    entropy_aleatorics.append(expected_entropy(entropy_preds).mean())\n",
    "    entropy_epistemics.append(mutual_information(entropy_preds).mean())\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ddfd58ea-62f4-4462-88b3-36afde5b2c2c",
   "metadata": {},
   "source": [
    "dataset_size"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "73ec7561-cc35-42a8-aad3-cb9ec285c3fe",
   "metadata": {},
   "source": [
    "\n",
    "dataset_sizes = np.logspace(start=1, stop=10, base=2, num=20) \n",
    "\n",
    "plt.plot(dataset_sizes, disentangling_accuracies, label=\"Disentangling model\")\n",
    "plt.plot(dataset_sizes, entropy_accuracies, label=\"Entropy model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Dataset size\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d73ef61f-29d5-473c-99c8-198ac5581f37",
   "metadata": {},
   "source": [
    "\n",
    "plt.plot(dataset_sizes, disentangling_aleatorics, label=\"Disentangling model\")\n",
    "plt.plot(dataset_sizes, entropy_aleatorics, label=\"Entropy model\")\n",
    "plt.ylabel(\"Aleatoric uncertainty\")\n",
    "plt.xlabel(\"Dataset size\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "97fb63de-a313-4808-8980-c0e0faaafc6d",
   "metadata": {},
   "source": [
    "\n",
    "plt.plot(dataset_sizes, disentangling_epistemics / max(disentangling_epistemics), label=\"Disentangling model (normalised)\")\n",
    "plt.plot(dataset_sizes, entropy_epistemics / max(entropy_epistemics), label=\"Entropy model (normalised)\")\n",
    "plt.ylabel(\"Epistemic uncertainty\")\n",
    "plt.xlabel(\"Dataset size\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "66e35993-ddfe-4160-82eb-774599f79857",
   "metadata": {},
   "source": [
    "\n",
    "# plt.plot(dataset_sizes, disentangling_epistemics, label=\"Disentangling model\")\n",
    "plt.plot(dataset_sizes, entropy_epistemics, label=\"Entropy model\")\n",
    "plt.ylabel(\"Epistemic uncertainty\")\n",
    "plt.xlabel(\"Dataset size\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "77bbf2fe-0611-4698-9414-8cd689b8257e",
   "metadata": {},
   "source": [
    "plt.plot(dataset_sizes, disentangling_aleatorics, label=\"Aleatoric\")\n",
    "plt.plot(dataset_sizes, disentangling_epistemics, label=\"Epistemic\")\n",
    "plt.title(\"Multi-head disentangled uncertainties vs. dataset size\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Uncertainty\")\n",
    "plt.xlabel(\"Dataset size\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "471507f9-ed9e-4beb-aba2-22a7a3d77fa9",
   "metadata": {},
   "source": [
    "plt.plot(dataset_sizes, (entropy_aleatorics - min(entropy_aleatorics)) / max(entropy_aleatorics - min(entropy_aleatorics)), label=\"Aleatoric\")\n",
    "plt.plot(dataset_sizes, (entropy_epistemics - min(entropy_epistemics)) / max(entropy_epistemics -min(entropy_epistemics) ), label=\"Epistemic\")\n",
    "plt.title(\"Entropy disentangled uncertainties vs. dataset size\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Uncertainty (normalised)\")\n",
    "plt.xlabel(\"Dataset size\")\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
